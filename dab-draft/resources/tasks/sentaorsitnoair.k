{
    'condition_task': {'anyOf': [{'type': 'object'}]}
}
#                'properties': {'left': {'type': 'string',
#                  'description': 'The left operand of the condition task. Can be either a string value or a job state or parameter reference.'},
#                 'op': {'type': 'string',
#                  'description': '* `EQUAL_TO`, `NOT_EQUAL` operators perform string comparison of their operands. This means that `“12.0” == “12”` will evaluate to `false`.\n* `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL` operators perform numeric comparison of their operands. `“12.0” >= “12”` will evaluate to `true`, `“10.0” >= “12”` will evaluate to `false`.\n\nThe boolean comparison to task values can be implemented with operators `EQUAL_TO`, `NOT_EQUAL`. If a task value was set to a boolean value, it will be serialized to `“true”` or `“false”` for the comparison.',
#                  'enum': ['EQUAL_TO',
#                   'GREATER_THAN',
#                   'GREATER_THAN_OR_EQUAL',
#                   'LESS_THAN',
#                   'LESS_THAN_OR_EQUAL',
#                   'NOT_EQUAL']},
#                 'right': {'type': 'string',
#                  'description': 'The right operand of the condition task. Can be either a string value or a job state or parameter reference.'}},
#                'additionalProperties': False,
#                'required': ['left', 'op', 'right']},
#               {'type': 'string',
#                'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#              'description': 'The task evaluates a condition that can be used to control the execution of other tasks when the `condition_task` field is present.\nThe condition task does not require a cluster to execute and does not support retries or notifications.'},
#             'dbt_task': {'anyOf': [{'type': 'object',
#                'properties': {'catalog': {'type': 'string',
#                  'description': 'Optional name of the catalog to use. The value is the top level in the 3-level namespace of Unity Catalog (catalog / schema / relation). The catalog value can only be specified if a warehouse_id is specified. Requires dbt-databricks >= 1.1.1.'},
#                 'commands': {'anyOf': [{'type': 'array',
#                    'items': {'type': 'string'}},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#                  'description': 'A list of dbt commands to execute. All commands must start with `dbt`. This parameter must not be empty. A maximum of up to 10 commands can be provided.'},
#                 'profiles_directory': {'type': 'string',
#                  'description': 'Optional (relative) path to the profiles directory. Can only be specified if no warehouse_id is specified. If no warehouse_id is specified and this folder is unset, the root directory is used.'},
#                 'project_directory': {'type': 'string',
#                  'description': 'Path to the project directory. Optional for Git sourced tasks, in which\ncase if no value is provided, the root of the Git repository is used.'},
#                 'schema': {'type': 'string',
#                  'description': 'Optional schema to write to. This parameter is only used when a warehouse_id is also provided. If not provided, the `default` schema is used.'},
#                 'source': {'type': 'string',
#                  'description': 'Optional location type of the project directory. When set to `WORKSPACE`, the project will be retrieved\nfrom the local Databricks workspace. When set to `GIT`, the project will be retrieved from a Git repository\ndefined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n\n* `WORKSPACE`: Project is located in Databricks workspace.\n* `GIT`: Project is located in cloud Git provider.',
#                  'enum': ['WORKSPACE', 'GIT']},
#                 'warehouse_id': {'type': 'string',
#                  'description': 'ID of the SQL warehouse to connect to. If provided, we automatically generate and provide the profile and connection details to dbt. It can be overridden on a per-command basis by using the `--profiles-dir` command line argument.'}},
#                'additionalProperties': False,
#                'required': ['commands']},
#               {'type': 'string',
#                'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#              'description': 'The task runs one or more dbt commands when the `dbt_task` field is present. The dbt task requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.'},
#             'for_each_task': {'anyOf': [{'type': 'object',
#                'properties': {'concurrency': {'anyOf': [{'type': 'integer'},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(resources(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(bundle(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(workspace(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(artifacts(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#                  'description': 'An optional maximum allowed number of concurrent runs of the task.\nSet this value if you want to be able to execute multiple runs of the task concurrently.'},
#                 'inputs': {'type': 'string',
#                  'description': 'Array for task to iterate on. This can be a JSON string or a reference to\nan array parameter.'},
#                 'task': {'anyOf': [...],
#                  'description': 'Configuration for the task that will be run for each element in the array'}},
#                'additionalProperties': False,
#                'required': ['inputs', 'task']},
#               {'type': 'string',
#                'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#              'description': 'The task executes a nested task for every input provided when the `for_each_task` field is present.'},
#             'notebook_task': {'anyOf': [{'type': 'object',
#                'properties': {'base_parameters': {'anyOf': [{'type': 'object',
#                    'additionalProperties': {'type': 'string'}},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#                  'description': 'Base parameters to be used for each run of this job. If the run is initiated by a call to :method:jobs/run\nNow with parameters specified, the two parameters maps are merged. If the same key is specified in\n`base_parameters` and in `run-now`, the value from `run-now` is used.\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nIf the notebook takes a parameter that is not specified in the job’s `base_parameters` or the `run-now` override parameters,\nthe default value from the notebook is used.\n\nRetrieve these parameters in a notebook using [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-widgets).\n\nThe JSON representation of this field cannot exceed 1MB.'},
#                 'notebook_path': {'type': 'string',
#                  'description': 'The path of the notebook to be run in the Databricks workspace or remote repository.\nFor notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash.\nFor notebooks stored in a remote repository, the path must be relative. This field is required.'},
#                 'source': {'type': 'string',
#                  'description': 'Optional location type of the notebook. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository\ndefined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n* `WORKSPACE`: Notebook is located in Databricks workspace.\n* `GIT`: Notebook is located in cloud Git provider.',
#                  'enum': ['WORKSPACE', 'GIT']},
#                 'warehouse_id': {'type': 'string',
#                  'description': 'Optional `warehouse_id` to run the notebook on a SQL warehouse. Classic SQL warehouses are NOT supported, please use serverless or pro SQL warehouses.\n\nNote that SQL warehouses only support SQL cells; if the notebook contains non-SQL cells, the run will fail.'}},
#                'additionalProperties': False,
#                'required': ['notebook_path']},
#               {'type': 'string',
#                'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#              'description': 'The task runs a notebook when the `notebook_task` field is present.'},
#             'python_wheel_task': {'anyOf': [{'type': 'object',
#                'properties': {'entry_point': {'type': 'string',
#                  'description': 'Named entry point to use, if it does not exist in the metadata of the package it executes the function from the package directly using `$packageName.$entryPoint()`'},
#                 'named_parameters': {'anyOf': [{'type': 'object',
#                    'additionalProperties': {'type': 'string'}},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#                  'description': 'Command-line parameters passed to Python wheel task in the form of `["--name=task", "--data=dbfs:/path/to/data.json"]`. Leave it empty if `parameters` is not null.'},
#                 'package_name': {'type': 'string',
#                  'description': 'Name of the package to execute'},
#                 'parameters': {'anyOf': [{'type': 'array',
#                    'items': {'type': 'string'}},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#                  'description': 'Command-line parameters passed to Python wheel task. Leave it empty if `named_parameters` is not null.'}},
#                'additionalProperties': False,
#                'required': ['entry_point', 'package_name']},
#               {'type': 'string',
#                'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#              'description': 'The task runs a Python wheel when the `python_wheel_task` field is present.'},
#             'spark_jar_task': {'anyOf': [{'type': 'object',
#                'properties': {'jar_uri': {'type': 'string',
#                  'description': 'Deprecated since 04/2016. Provide a `jar` through the `libraries` field instead. For an example, see :method:jobs/create.'},
#                 'main_class_name': {'type': 'string',
#                  'description': 'The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library.\n\nThe code must use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job fail.'},
#                 'parameters': {'anyOf': [{'type': 'array',
#                    'items': {'type': 'string'}},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#                  'description': 'Parameters passed to the main method.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.'}},
#                'additionalProperties': False},
#               {'type': 'string',
#                'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#              'description': 'The task runs a JAR when the `spark_jar_task` field is present.'},
#             'spark_submit_task': {'anyOf': [{'type': 'object',
#                'properties': {'parameters': {'anyOf': [{'type': 'array',
#                    'items': {'type': 'string'}},
#                   {'type': 'string',
#                    'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#                  'description': 'Command-line parameters passed to spark submit.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.'}},
#                'additionalProperties': False},
#               {'type': 'string',
#                'pattern': '\\$\\{(var(\\.[a-zA-Z]+([-_]?[a-zA-Z0-9]+)*(\\[[0-9]+\\])*)+)\\}'}],
#              'description': '(Legacy) The task runs the spark-submit script when the `spark_submit_task` field is present. This task can run only on new clusters and is not compatible with serverless compute.\n\nIn the `new_cluster` specification, `libraries` and `spark_conf` are not supported. Instead, use `--jars` and `--py-files` to add Java and Python libraries and `--conf` to set the Spark configurations.\n\n`master`, `deploy-mode`, and `executor-cores` are automatically configured by Databricks; you _cannot_ specify them in parameters.\n\nBy default, the Spark submit job uses all available memory (excluding reserved memory for Databricks services). You can set `--driver-memory`, and `--executor-memory` to a smaller value to leave some room for off-heap usage.\n\nThe `--jars`, `--py-files`, `--files` arguments support DBFS and S3 paths.'},
